{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma for eps=0.5\n",
      "DP-SGD with sampling rate = 0.136% and noise_multiplier = 3.41 iterated over 52805 steps satisfies differential privacy with eps = 0.5 and delta = 1e-06.\n",
      "The optimal RDP order is 57.0.\n",
      "DP-SGD with sampling rate = 0.17% and noise_multiplier = 3.795 iterated over 42244 steps satisfies differential privacy with eps = 0.5 and delta = 1e-06.\n",
      "The optimal RDP order is 57.0.\n",
      "\n",
      "Sigma for eps=1\n",
      "DP-SGD with sampling rate = 0.136% and noise_multiplier = 1.82 iterated over 52805 steps satisfies differential privacy with eps = 1 and delta = 1e-06.\n",
      "The optimal RDP order is 29.0.\n",
      "DP-SGD with sampling rate = 0.17% and noise_multiplier = 2.01 iterated over 42244 steps satisfies differential privacy with eps = 1 and delta = 1e-06.\n",
      "The optimal RDP order is 29.0.\n",
      "\n",
      "Sigma for eps=2\n",
      "DP-SGD with sampling rate = 0.136% and noise_multiplier = 1.09 iterated over 52805 steps satisfies differential privacy with eps = 2 and delta = 1e-06.\n",
      "The optimal RDP order is 14.0.\n",
      "DP-SGD with sampling rate = 0.17% and noise_multiplier = 1.175 iterated over 42244 steps satisfies differential privacy with eps = 2 and delta = 1e-06.\n",
      "The optimal RDP order is 15.0.\n",
      "\n",
      "Sigma for eps=4\n",
      "DP-SGD with sampling rate = 0.136% and noise_multiplier = 0.778 iterated over 52805 steps satisfies differential privacy with eps = 4 and delta = 1e-06.\n",
      "The optimal RDP order is 7.0.\n",
      "DP-SGD with sampling rate = 0.17% and noise_multiplier = 0.811 iterated over 42244 steps satisfies differential privacy with eps = 4 and delta = 1e-06.\n",
      "The optimal RDP order is 7.0.\n",
      "\n",
      "Sigma for eps=8\n",
      "DP-SGD with sampling rate = 0.136% and noise_multiplier = 0.6173 iterated over 52805 steps satisfies differential privacy with eps = 8 and delta = 1e-06.\n",
      "The optimal RDP order is 4.0.\n",
      "DP-SGD with sampling rate = 0.17% and noise_multiplier = 0.637 iterated over 42244 steps satisfies differential privacy with eps = 8 and delta = 1e-06.\n",
      "The optimal RDP order is 4.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.001949867989007, 4.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-dp training converges at epoch 72\n",
    "\n",
    "print('Sigma for eps=0.5')\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=751000, batch_size=1024, noise_multiplier=3.41, epochs=72, delta=1e-6)\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=300400, batch_size=512, noise_multiplier=3.795, epochs=72, delta=1e-6)\n",
    "\n",
    "print('\\nSigma for eps=1')\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=751000, batch_size=1024, noise_multiplier=1.82, epochs=72, delta=1e-6)\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=300400, batch_size=512, noise_multiplier=2.01, epochs=72, delta=1e-6)\n",
    "\n",
    "print('\\nSigma for eps=2')\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=751000, batch_size=1024, noise_multiplier=1.09, epochs=72, delta=1e-6)\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=300400, batch_size=512, noise_multiplier=1.175, epochs=72, delta=1e-6)\n",
    "\n",
    "print('\\nSigma for eps=4')\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=751000, batch_size=1024, noise_multiplier=0.778, epochs=72, delta=1e-6)\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=300400, batch_size=512, noise_multiplier=0.811, epochs=72, delta=1e-6)\n",
    "\n",
    "print('\\nSigma for eps=8')\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=751000, batch_size=1024, noise_multiplier=0.6173, epochs=72, delta=1e-6)\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=300400, batch_size=512, noise_multiplier=0.637, epochs=72, delta=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/liyuan/dlrm/venv/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liyuan/dlrm/venv/lib/python3.6/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "cuda\n",
      "CASIA train: 751000 1467\n",
      "HIT train: 300400 587\n",
      "CASIA val: 146758 287\n",
      "HIT val: 67590 133\n",
      "batch per lot: 2 1\n",
      "Epsilon: 1 Sigma: 1.82 2.01\n"
     ]
    }
   ],
   "source": [
    "# HIT & CASIA PyTorch training with FederatedAveraging\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import h5py as h5\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "eps2sigma = {\n",
    "    0.5: (3.41, 3.795),\n",
    "    1: (1.82, 2.01),\n",
    "    2: (1.09, 1.175),\n",
    "    4: (0.778, 0.811),\n",
    "    8: (0.6173, 0.637)\n",
    "}\n",
    "        \n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 512\n",
    "        self.val_batch_size = 512\n",
    "        self.epochs = 73\n",
    "        self.lr = 0.0001\n",
    "        self.gpu = True and torch.cuda.is_available()\n",
    "        self.seed = 0\n",
    "        self.log_interval = 200\n",
    "        self.checkpoint = 5\n",
    "        ### DP config ###\n",
    "        self.epsilon = 1  # epsilon for each party\n",
    "        self.clip = 1  # gradient clip L2 bound\n",
    "        self.delta = 1e-6\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if args.gpu else \"cpu\")\n",
    "print(device)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.gpu else {}\n",
    "\n",
    "# load data\n",
    "class HIT(torch.utils.data.Dataset):\n",
    "    # group: trn/vld\n",
    "    def __init__(self, archive, group, transform=None):\n",
    "        self.archive = h5.File(archive, 'r')\n",
    "        self.x = self.archive[group + '/x']\n",
    "        self.y = self.archive[group + '/y']\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        datum = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            datum = self.transform(datum)\n",
    "        label = self.y[index][0].astype('int64')\n",
    "        return datum, label\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def close(self):\n",
    "        self.archive.close()\n",
    "\n",
    "def to_tensor(img):\n",
    "    img = torch.from_numpy(img)\n",
    "    return img.float().div(255)\n",
    "tfm = transforms.Lambda(to_tensor)\n",
    "\n",
    "trainset_casia = HIT('mdata/HWDB1.1fullset.hdf5', 'trn', transform=transforms.Compose([tfm]))\n",
    "trainset_hit = HIT('mdata/HIT_OR3Cfullset.hdf5', 'trn', transform=transforms.Compose([tfm]))\n",
    "# sequential loader\n",
    "# train_loader_casia = DataLoader(trainset_casia, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "# train_loader_hit = DataLoader(trainset_hit, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "# random loader\n",
    "sampler_casia = RandomSampler(trainset_casia, replacement=True)\n",
    "sampler_hit = RandomSampler(trainset_hit, replacement=True)\n",
    "train_loader_casia = DataLoader(trainset_casia, batch_size=args.batch_size, shuffle=False, sampler=sampler_casia, **kwargs)\n",
    "train_loader_hit = DataLoader(trainset_hit, batch_size=args.batch_size, shuffle=False, sampler=sampler_hit, **kwargs)\n",
    "\n",
    "valset_casia = HIT('mdata/HWDB1.1fullset.hdf5', 'vld', transform=transforms.Compose([tfm]))\n",
    "valset_hit = HIT('mdata/HIT_OR3Cfullset.hdf5', 'vld', transform=transforms.Compose([tfm]))\n",
    "val_loader_casia = DataLoader(valset_casia, batch_size=args.val_batch_size, shuffle=False, **kwargs)\n",
    "val_loader_hit = DataLoader(valset_hit, batch_size=args.val_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "print('CASIA train:', len(trainset_casia), len(train_loader_casia))\n",
    "print('HIT train:', len(trainset_hit), len(train_loader_hit))\n",
    "print('CASIA val:', len(valset_casia), len(val_loader_casia))\n",
    "print('HIT val:', len(valset_hit), len(val_loader_hit))\n",
    "\n",
    "### DP config ###\n",
    "batch_per_lot_casia = max(round(len(trainset_casia)**.5 / args.batch_size), 1)  # set lotsize = sqrt(N)\n",
    "batch_per_lot_hit = max(round(len(trainset_hit)**.5 / args.batch_size), 1)\n",
    "print('batch per lot:', batch_per_lot_casia, batch_per_lot_hit)\n",
    "delta = 10**(-5)\n",
    "sigma_casia, sigma_hit = eps2sigma[args.epsilon]\n",
    "print('Epsilon:', args.epsilon, 'Sigma:', sigma_casia, sigma_hit)\n",
    "\n",
    "# Model\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes, batch_per_lot=None, sigma=None):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.batch_per_lot = batch_per_lot  # for DP\n",
    "        self.sigma = sigma  # for DP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(1024, momentum=0.66),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(256, momentum=0.66),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def divide_clip_grads(self):\n",
    "        for key, param in self.named_parameters():\n",
    "            param.grad /= self.batch_per_lot\n",
    "            nn.utils.clip_grad_norm([param], args.clip)\n",
    "\n",
    "    def gaussian_noise(self, grads):\n",
    "        shape = grads.shape\n",
    "        noise = Variable(torch.zeros(shape))\n",
    "        noise = noise.to(device)\n",
    "        noise.data.normal_(0.0, std=args.clip*self.sigma)\n",
    "        return noise\n",
    "            \n",
    "    def add_noise_to_grads(self):\n",
    "        for key, param in self.named_parameters():\n",
    "            lotsize = self.batch_per_lot * args.batch_size\n",
    "            noise = 1/lotsize * self.gaussian_noise(param.grad)\n",
    "            param.grad += noise\n",
    "\n",
    "def conv_unit(input, output, mp=False):\n",
    "    if mp:\n",
    "        return [nn.Conv2d(input, output, kernel_size=3, stride=1, padding=1), nn.LeakyReLU(), \n",
    "               nn.BatchNorm2d(output, momentum=0.66), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]\n",
    "    else:\n",
    "        return [nn.Conv2d(input, output, kernel_size=3, stride=1, padding=1), nn.LeakyReLU(), \n",
    "               nn.BatchNorm2d(output, momentum=0.66)]\n",
    "\n",
    "def make_layers():\n",
    "    layers = []\n",
    "    layers += [nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1), nn.LeakyReLU(), \n",
    "               nn.BatchNorm2d(64, momentum=0.66)]\n",
    "\n",
    "    layers += conv_unit(64, 128)\n",
    "    layers += conv_unit(128, 128, mp=True)\n",
    "\n",
    "    layers += conv_unit(128, 256)\n",
    "    layers += conv_unit(256, 256, mp=True)\n",
    "\n",
    "    layers += conv_unit(256, 384)\n",
    "    layers += conv_unit(384, 384)\n",
    "    layers += conv_unit(384, 384, mp=True)\n",
    "\n",
    "    layers += conv_unit(384, 512)\n",
    "    layers += conv_unit(512, 512)\n",
    "    layers += conv_unit(512, 512, mp=True)\n",
    "\n",
    "    layers += [nn.Flatten()]\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# use PySyft for SPDZ\n",
    "hook = sy.TorchHook(torch)\n",
    "casia = sy.VirtualWorker(hook, id=\"casia\")\n",
    "hit = sy.VirtualWorker(hook, id=\"hit\")\n",
    "crypto = sy.VirtualWorker(hook, id=\"crypto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Uncomment SWITCH ON to switch on PySyft remote training. Not working for now.\n",
    "\n",
    "# SWITCH ON\n",
    "# compute_nodes = [casia, hit]\n",
    "# remote_loader_casia = []\n",
    "# remote_loader_hit = []\n",
    "\n",
    "# for batch_idx, (data,target) in enumerate(train_loader_casia):\n",
    "#     data = data.send(casia)\n",
    "#     target = target.send(casia)\n",
    "#     remote_loader_casia.append((data, target))\n",
    "\n",
    "# for batch_idx, (data,target) in enumerate(train_loader_hit):\n",
    "#     data = data.send(hit)\n",
    "#     target = target.send(hit)\n",
    "#     remote_loader_hit.append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72\n",
      "Val loss_casia 0.2845, loss_hit 0.1115, acc_casia 0.9363, acc_hit 0.9770\n"
     ]
    }
   ],
   "source": [
    "model_casia = VGG(make_layers(), 3755)  # normal model\n",
    "model_casia = VGG(make_layers(), 3755, batch_per_lot_casia, sigma_casia)  # dp model\n",
    "epo = -1\n",
    "\n",
    "epo = 72  # load trained model\n",
    "model_casia.load_state_dict(torch.load('models/smpc-hit-casia-{:d}.pt'.format(epo)))  # continue from previous training\n",
    "\n",
    "model_hit = copy.deepcopy(model_casia)\n",
    "\n",
    "model_hit.batch_per_lot = batch_per_lot_hit  # dp model\n",
    "model_hit.sigma = sigma_hit\n",
    "\n",
    "model_casia = model_casia.to(device)\n",
    "model_hit = model_hit.to(device)\n",
    "\n",
    "optim_casia = optim.Adam(model_casia.parameters(), lr=args.lr)\n",
    "optim_hit = optim.Adam(model_hit.parameters(), lr=args.lr)\n",
    "\n",
    "models = [model_casia, model_hit]\n",
    "params = [list(model_casia.parameters()), list(model_hit.parameters())]\n",
    "optims = [optim_casia, optim_hit]\n",
    "\n",
    "# models[0].send(compute_nodes[0])  # SWITCH ON\n",
    "# models[1].send(compute_nodes[1])\n",
    "\n",
    "def train(epoch):\n",
    "    \n",
    "    assert len(params[0]) == len(params[1])\n",
    "    for param_index in range(len(params[0])):\n",
    "        assert torch.equal(params[0][param_index], params[1][param_index])\n",
    "\n",
    "    models[0].train()\n",
    "    models[1].train()\n",
    "    \n",
    "    losses = [0, 0]\n",
    "    corrects = [0, 0]\n",
    "    \n",
    "    def update(data, target, model, optimizer, party, batch_i=None, batch_per_lot=None):\n",
    "        global counter\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        pred = output.argmax(dim=1, keepdim=True)    # get the index of the max log-probability\n",
    "        if party == 'casia':\n",
    "            corrects[0] += pred.eq(target.view_as(pred)).sum().item()  # debug SWITCH OFF\n",
    "            losses[0] += F.nll_loss(output, target, reduction='sum').item()\n",
    "#             corrects[0] += pred.eq(target.view_as(pred)).sum().get().item()  # SWITCH ON\n",
    "#             losses[0] += F.nll_loss(output, target, reduction='sum').get().item()\n",
    "        else:\n",
    "            corrects[1] += pred.eq(target.view_as(pred)).sum().item()  # debug SWITCH OFF\n",
    "            losses[1] += F.nll_loss(output, target, reduction='sum').item()\n",
    "#             corrects[1] += pred.eq(target.view_as(pred)).sum().get().item()  # SWITCH ON\n",
    "#             losses[1] += F.nll_loss(output, target, reduction='sum').get().item()\n",
    "        loss.backward()\n",
    "        \n",
    "        if batch_per_lot:  # dp update\n",
    "            if batch_i % batch_per_lot == batch_per_lot - 1:\n",
    "                model.divide_clip_grads()\n",
    "                model.add_noise_to_grads()\n",
    "                optimizer.step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            \n",
    "    for batch_i, (data, target) in enumerate(train_loader_casia):  # SWITCH OFF\n",
    "#         update(data, target, models[0], optims[0], 'casia')\n",
    "        update(data, target, models[0], optims[0], 'casia', batch_i, batch_per_lot_casia)  # dp update\n",
    "    for batch_i, (data, target) in enumerate(train_loader_hit):  # SWITCH OFF\n",
    "#         update(data, target, models[1], optims[1], 'hit')\n",
    "        update(data, target, models[1], optims[1], 'hit', batch_i, batch_per_lot_hit)\n",
    "#     for batch_i, (data, target) in enumerate(remote_loader_casia):  # SWITCH ON\n",
    "#         update(data, target, models[0], optims[0], 'casia')\n",
    "#     for batch_i, (data, target) in enumerate(remote_loader_hit):  # SWITCH ON\n",
    "#         update(data, target, models[1], optims[1], 'hit')\n",
    "    \n",
    "    loss_casia, loss_hit = losses[0], losses[1]\n",
    "    correct_casia, correct_hit = corrects[0], corrects[1]\n",
    "    \n",
    "    loss_casia /= len(trainset_casia)\n",
    "    loss_hit /= len(trainset_hit)\n",
    "    acc_casia = correct_casia / len(trainset_casia)\n",
    "    acc_hit = correct_hit / len(trainset_hit)\n",
    "    print('Trn loss_casia {:.4f}, loss_hit {:.4f}, acc_casia {:.4f}, acc_hit {:.4f}'.format(loss_casia, loss_hit, acc_casia, acc_hit))\n",
    "    \n",
    "    ratio_casia = 25\n",
    "    ratio_hit = 10\n",
    "    \n",
    "    # FedAvg using SPDZ\n",
    "    new_params = list()\n",
    "    for param_i in range(len(params[0])):\n",
    "        spdz_params = list()\n",
    "        spdz_params.append(params[0][param_i].copy().cpu().fix_precision().share(casia, hit, crypto_provider=crypto))\n",
    "        spdz_params.append(params[1][param_i].copy().cpu().fix_precision().share(casia, hit, crypto_provider=crypto))\n",
    "#         if str(device) == 'cpu':  SWITCH ON\n",
    "              # see https://github.com/OpenMined/PySyft/pull/2990\n",
    "#             spdz_params.append(params[0][param_i].copy().get().fix_precision().share(casia, hit, crypto_provider=crypto))\n",
    "#             spdz_params.append(params[1][param_i].copy().get().fix_precision().share(casia, hit, crypto_provider=crypto))\n",
    "#         else:\n",
    "#             spdz_params.append(params[0][param_i].copy().cpu().get().fix_precision().share(casia, hit, crypto_provider=crypto))\n",
    "#             spdz_params.append(params[1][param_i].copy().cpu().get().fix_precision().share(casia, hit, crypto_provider=crypto))\n",
    "\n",
    "        new_param = (spdz_params[0] * ratio_casia + spdz_params[1] * ratio_hit).get().float_precision() / (ratio_casia + ratio_hit)\n",
    "        new_params.append(new_param)\n",
    "\n",
    "    # cleanup\n",
    "    with torch.no_grad():\n",
    "        for model in params:\n",
    "            for param in model:\n",
    "                param *= 0\n",
    "\n",
    "#         for model in models:  # SWITCH ON\n",
    "#             model.get()\n",
    "\n",
    "        for param_index in range(len(params[0])):\n",
    "            if str(device) == 'cpu':\n",
    "                params[0][param_index].set_(new_params[param_index])\n",
    "                params[1][param_index].set_(new_params[param_index])\n",
    "            else:\n",
    "                params[0][param_index].set_(new_params[param_index].cuda())\n",
    "                params[1][param_index].set_(new_params[param_index].cuda())\n",
    "    \n",
    "    # FedAvg without SPDZ\n",
    "#     with torch.no_grad():\n",
    "#         for p1, p2 in zip(models[0].parameters(), models[1].parameters()):\n",
    "#             p1.set_((p1.data * ratio_casia + p2.data * ratio_hit) / (ratio_casia + ratio_hit))\n",
    "#             p2.set_(p1.data)\n",
    "\n",
    "def val():\n",
    "    assert len(params[0]) == len(params[1])\n",
    "    for param_index in range(len(params[0])):\n",
    "        assert torch.equal(params[0][param_index], params[1][param_index])\n",
    "    \n",
    "#     model_casia.eval()  # doesn't work right\n",
    "    losses = [0, 0]\n",
    "    corrects = [0, 0]\n",
    "    \n",
    "    def val_batch(data, target, model, party):\n",
    "        data, target = data.to(device), target.to(device)  # dev\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)    # get the index of the max log-probability\n",
    "        if party == 'casia':\n",
    "            corrects[0] += pred.eq(target.view_as(pred)).sum().item()\n",
    "            losses[0] += F.nll_loss(output, target, reduction='sum').item()\n",
    "        else:\n",
    "            corrects[1] += pred.eq(target.view_as(pred)).sum().item()\n",
    "            losses[1] += F.nll_loss(output, target, reduction='sum').item()\n",
    "    \n",
    "    for data, target in val_loader_casia:\n",
    "        val_batch(data, target, models[0], 'casia')\n",
    "    for data, target in val_loader_hit:\n",
    "        val_batch(data, target, models[1], 'hit')\n",
    "    \n",
    "    loss_casia, loss_hit = losses[0], losses[1]\n",
    "    correct_casia, correct_hit = corrects[0], corrects[1]\n",
    "    \n",
    "    loss_casia /= len(valset_casia)\n",
    "    loss_hit /= len(valset_hit)\n",
    "    acc_casia = correct_casia / len(valset_casia)\n",
    "    acc_hit = correct_hit / len(valset_hit)\n",
    "    print('Val loss_casia {:.4f}, loss_hit {:.4f}, acc_casia {:.4f}, acc_hit {:.4f}'.format(loss_casia, loss_hit, acc_casia, acc_hit))\n",
    "\n",
    "# Train\n",
    "start_epoch = epo + 1\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    print('Epoch', epoch)\n",
    "    t1 = int(time.time())\n",
    "    train(epoch)\n",
    "    t2 = int(time.time())\n",
    "    val()\n",
    "    t3 = int(time.time())\n",
    "    print('Epoch trn time {:d}s, val time {:d}s'.format(t2-t1, t3-t2))\n",
    "    torch.save(models[0].state_dict(), \"models/smpc-hit-casia-{}.pt\".format(epoch))\n",
    "\n",
    "torch.save(models[0].state_dict(), \"models/smpc-hit-casia.pt\")\n",
    "\n",
    "# Val\n",
    "print('Epoch', epo)\n",
    "val()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
